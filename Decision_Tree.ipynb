{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "- What is a Decision Tree?\n",
        "  - A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In classification, it is used to predict categorical outcomes (e.g., \"Yes\"/\"No\", \"Spam\"/\"Not Spam\", \"Disease\"/\"No Disease\").It resembles a tree-like structure where :\n",
        "    - Internal nodes represent decision rules (e.g., Age < 30?)\n",
        "    - Branches represent outcomes of these rules (e.g., Yes/No)\n",
        "    - Leaf nodes represent final class labels\n",
        "- How it Works in Classification :\n",
        "  - Start at the Root Node : Begin with the entire dataset.\n",
        "  - Choose the Best Feature to Split : Use algorithms like Gini Index, Entropy & Information Gain, or Chi-Square to find the best feature that splits the data into distinct classes.\n",
        "  - Split the Dataset : Based on the selected feature, split the dataset into subsets.\n",
        "  - Repeat Recursively : Apply the same process to each subset, building branches and further nodes until :\n",
        "    - All data in a node belongs to one class, or\n",
        "    - A maximum depth is reached, or\n",
        "    - No further information gain can be achieved\n",
        "  - Make Prediction : To classify a new input, traverse the tree from root to leaf, following decisions based on input features.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "- What Are Impurity Measures?\n",
        "  - In a Decision Tree, impurity measures help determine how â€œmixedâ€ or impure a node is. A node is pure if all its samples belong to a single class.\n",
        "  - The two most common impurity measures are :\n",
        "    - Gini Impurity\n",
        "    - Entropy (used with Information Gain)\n",
        "  - These are used to evaluate the quality of a split. A split is good if it reduces impurity â€” i.e., results in more â€œpureâ€ child nodes.\n",
        "1. Gini Impurity\n",
        "  - Definition :\n",
        "    - Gini(ð·)=1âˆ’âˆ‘ð‘–=1 to ð¶(ð‘ð‘–2)\n",
        "      - Where :\n",
        "        - ð¶ = number of classes\n",
        "        - ð‘ð‘– = proportion of samples in class\n",
        "  - Interpretation : Ranges from 0 (pure) to (1 - 1/C)\n",
        "     - If all records in a node belong to one class â†’ Gini = 0 (ideal case)\n",
        "     - The lower the Gini, the better the split\n",
        "  - Example : If a node contains :\n",
        "     - 60% Class A â†’ ð‘1=0.6\n",
        "     - 40% Class B â†’ ð‘2=0.4\n",
        "     - Gini = 1âˆ’(0.62+0.42)=1âˆ’(0.36+0.16)=0.48\n",
        "2. Entropy (Information Gain)\n",
        "  - Definition :\n",
        "    - Entropy(ð·)=âˆ’âˆ‘ð‘–=1ð¶ð‘ð‘–logâ¡2(ð‘ð‘–)\n",
        "    - Measures the amount of uncertainty in the dataset\n",
        "    - Used to calculate Information Gain\n",
        "    - Information Gain = Entropy (Parent Node) - Weighted Entropy (Children)\n",
        "  - Interpretation :\n",
        "    - Entropy is 0 when node is pure\n",
        "    - Higher entropy means more disorder/mixed classes\n",
        "  - Example : Same class proportions :\n",
        "    - ð‘1=0.6p1=0.6,ð‘2=0.4\n",
        "    - Entropy =âˆ’(0.6logâ¡20.6+0.4logâ¡20.4)â‰ˆ0.971\n",
        "- Impact on Decision Tree Splits\n",
        "  - When building a tree : For each feature, the algorithm tries all possible split points\n",
        "  - It calculates Gini or Entropy for the resulting child nodes\n",
        "  - Best split = one that minimizes Gini or maximizes Information Gain\n",
        "  - This process repeats recursively to grow the tree\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "- What is Pruning in Decision Trees? : Pruning is the process of reducing the size of a decision tree by removing unnecessary branches that may lead to overfitting.\n",
        "- There are two main types of pruning:\n",
        "1. Pre-Pruning (Early Stopping)\n",
        "- Definition : Stops the tree from growing too deep during construction.\n",
        "- How it works : The algorithm does not split a node if :\n",
        "  - The number of instances is below a threshold (e.g., min_samples_split)\n",
        "  - The depth exceeds a limit (max_depth)\n",
        "  - The information gain is too small\n",
        "- Practical Advantage :\n",
        "  - Faster training and simpler trees\n",
        "  - Useful when working with large datasets or limited computing resources.\n",
        "2. Post-Pruning (Reduced Error Pruning)\n",
        "- Definition : The tree is fully grown first, then unnecessary branches are pruned back.\n",
        "- How it works : After building the tree:\n",
        "  - Evaluate subtrees on a validation set\n",
        "  - Remove nodes if their removal does not reduce prediction accuracy\n",
        "- Practical Advantage :\n",
        "  - Better generalization and more accurate models\n",
        "  - Useful when you want to minimize overfitting after tree construction.\n",
        "- Pre-Pruning vs. Post-Pruning (Comparison)\n",
        "1. When It Is Applied :\n",
        "  - Pre-Pruning : Applied during the construction of the decision tree.\n",
        "  - Post-Pruning : Applied after the full tree has been built.\n",
        "2. Purpose :\n",
        "  - Pre-Pruning : To stop the tree early to avoid overfitting.\n",
        "  - Post-Pruning : To cut back the overfitted parts of a fully grown tree.\n",
        "3. How It Works  :\n",
        "  - Pre-Pruning : Uses conditions like max_depth, min_samples_split, or min_gain to prevent further splits.\n",
        "  - Post-Pruning: Evaluates each subtree using a validation set and prunes if it doesn't improve performance.\n",
        "4. Risk of Underfitting :\n",
        "  - Pre-Pruning : High risk â€” might underfit the data if stopped too early.\n",
        "  - Post-Pruning : Low risk â€” starts with a complex tree, then simplifies only where needed.\n",
        "5. Computation Time :\n",
        "  - Pre-Pruning : Faster, as it avoids growing unnecessary branches.\n",
        "  - Post-Pruning : Slower, as it builds a full tree and then evaluates multiple pruning options.\n",
        "6. Model Complexity :\n",
        "  - Pre-Pruning : Produces simpler and smaller trees.\n",
        "  - Post-Pruning : Allows complex trees initially, but results in a balanced, optimized tree.\n",
        "7. Control Parameters\n",
        "  - Pre-Pruning : Controlled by hyperparameters like max_depth, min_samples_leaf, etc.\n",
        "  - Post-Pruning : Controlled using pruning algorithms (like cost-complexity pruning) and a validation set.\n",
        "8. Typical Use Case :\n",
        "  - Pre-Pruning : Preferred when working with large datasets or time constraints.\n",
        "  - Post-Pruning : Preferred when model accuracy and generalization are the top priorities.\n",
        "9. Example in scikit-learn :\n",
        "  - Pre-Pruning : Use parameters like DecisionTreeClassifier(max_depth=3)\n",
        "  - Post-Pruning : Use ccp_alpha (Cost Complexity Pruning) for pruning after full growth.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "- What is Information Gain? : Information Gain (IG) is a metric used in Decision Trees to measure the effectiveness of an attribute (feature) in splitting the dataset into pure subsets (i.e., subsets where most or all instances belong to the same class).It is based on Entropy, which measures the impurity or disorder of the data.\n",
        "- Formula for Information Gain :\n",
        "  - InformationÂ Gain = EntropyÂ (Parent)âˆ’âˆ‘(ð‘›ð‘–ð‘›Ã—EntropyÂ (Childð‘–))\n",
        "    - Where :\n",
        "      - ð‘› = total samples in parent node\n",
        "      - ð‘›ð‘– = samples in each child node\n",
        "- Entropy is calculated using the class distribution in each node.\n",
        "- Why Is It Important? : Information Gain tells us how much â€œinformationâ€ a feature gives us about the class. A higher Information Gain means that the split on that feature reduces uncertainty the most.\n",
        "  - In simple terms :\n",
        "    - Higher IG = Better split = More pure subsets = Better tree decisions\n",
        "  - How It Helps in Choosing the Best Split :\n",
        "    - For each feature : Try all possible splits (like thresholds for numerical data or categories for categorical data).Calculate the Information Gain from each split.Choose the feature and threshold that results in the highest Information Gain.\n",
        "  - This helps the tree pick the most informative feature first, leading to faster convergence and better accuracy.\n",
        "- Example : Suppose you're classifying \"Play Tennis\" based on \"Outlook\":\n",
        "  - Entropy before split = 0.94\n",
        "  - After splitting on \"Outlook\", weighted entropy = 0.69\n",
        "  - InformationÂ Gain = 0.94âˆ’0.69=0.25\n",
        "  - So, splitting on \"Outlook\" gives an Information Gain of 0.25, which means this feature reduces uncertainty significantly.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "- Real-World Applications of Decision Trees\n",
        "  - Medical Diagnosis : Predicting whether a patient has a disease based on symptoms, age, test results, etc.Example: Diagnosing diabetes or heart disease.\n",
        "  - Loan Approval in Banking : Classifying loan applications as â€œapprovedâ€ or â€œrejectedâ€ based on income, credit score, and history.\n",
        "  - Customer Churn Prediction : Identifying which customers are likely to leave a service based on usage, feedback, and support interactions.\n",
        "  - Fraud Detection : Detecting fraudulent transactions using features like transaction amount, location, frequency, etc.\n",
        "  - Retail: Product Recommendation & Inventory Management.Classifying customers based on buying patterns to suggest products or manage stock.\n",
        "  - Manufacturing : Predicting equipment failure or quality control based on sensor data or machine logs.\n",
        "  - Education : Predicting student performance or drop-out risk using attendance, grades, and engagement metrics.\n",
        "- Advantages of Decision Trees\n",
        "  - Easy to Understand and Interpret : Resembles human decision-making; can be visualized and explained easily.\n",
        "  - Handles Both Numerical and Categorical Data : Works well with mixed-type features without needing complex preprocessing.\n",
        "  - No Need for Feature Scaling : Unlike SVM or KNN, normalization or standardization is not required.\n",
        "  - Can Handle Missing Values : Some implementations (e.g., in sklearn) can manage missing data during training.\n",
        "  - Works Well for Small to Medium Datasets : Especially when data has clear patterns or rules.\n",
        "- Limitations of Decision Trees\n",
        "  - Prone to Overfitting : Without pruning, decision trees can become too complex and memorize training data.\n",
        "  - Unstable to Small Changes : Small variations in data can lead to completely different trees (high variance).\n",
        "  - Biased Toward Features with More Levels : Features with many unique values may dominate splits.\n",
        "  - Limited Predictive Power Alone : Often less accurate than ensemble methods like Random Forest or Gradient Boosting.\n",
        "\n",
        "Question 10: Imagine youâ€™re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "â— Handle the missing values\n",
        "â— Encode the categorical features\n",
        "â— Train a Decision Tree model\n",
        "â— Tune its hyperparameters\n",
        "â— Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "- Youâ€™re a data scientist for a healthcare company using a dataset with mixed data types (numerical + categorical) and missing values to predict disease presence. Here's how you'd proceed:\n",
        "- Step 1 : Handle the Missing Values\n",
        "  - Understand the Missingness\n",
        "  - Identify how much data is missing and why (MCAR, MAR, MNAR).\n",
        "  - Use .isnull().sum() in pandas to check missing counts.\n",
        "  - Numerical Features\n",
        "    - Use mean/median imputation:\n",
        "      - from sklearn.impute import SimpleImputer\n",
        "      - num_imputer = SimpleImputer(strategy='median')\n",
        "      - df[numerical_cols] = num_imputer.fit_transform(df[numerical_cols])\n",
        "  - Categorical Features\n",
        "    - Use most frequent (mode) or a special category like \"Unknown\":\n",
        "      - cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "      - df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
        "- Step 2 : Encode the Categorical Features\n",
        "  - For Decision Trees, label encoding works fine (they don't require one-hot):\n",
        "     - from sklearn.preprocessing import OrdinalEncoder\n",
        "     - encoder = OrdinalEncoder()\n",
        "     - df[categorical_cols] = encoder.fit_transform(df[categorical_cols])\n",
        "  - Alternatively, use pd.get_dummies() if categories are unordered and not too many :\n",
        "     - df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "- Step 3 : Train a Decision Tree Model\n",
        "  - Split the data into train and test sets:\n",
        "    - from sklearn.model_selection import train_test_split\n",
        "    - X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  - Train the Decision Tree:\n",
        "    - from sklearn.tree import DecisionTreeClassifier\n",
        "    - model = DecisionTreeClassifier(random_state=42)\n",
        "    - model.fit(X_train, y_train)\n",
        "- Step 4 : Tune Hyperparameters\n",
        "  - Use GridSearchCV to find the best combination :\n",
        "     - from sklearn.model_selection import GridSearchCV\n",
        "     - param_grid = {\n",
        "         - 'max_depth': [3, 5, 10, None],\n",
        "         - 'min_samples_split': [2, 10, 20],\n",
        "         - 'min_samples_leaf': [1, 5, 10],\n",
        "         - 'criterion': ['gini', 'entropy']\n",
        "     - }\n",
        "    - grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                          param_grid, cv=5, scoring='accuracy')\n",
        "    - grid_search.fit(X_train, y_train)\n",
        "    - best_model = grid_search.best_estimator_\n",
        "Step 5 : Evaluate Model Performance\n",
        "  - Make predictions :\n",
        "    - y_pred = best_model.predict(X_test)\n",
        "  - Metrics :\n",
        "    - from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "    - print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    - print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    - print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    - print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "    - print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "- Business Value in Real-World Healthcare\n",
        "  - Early Disease Detection : Helps doctors flag high-risk patients early, even before symptoms appear.\n",
        "  - Resource Optimization : Hospitals can prioritize tests or treatments for patients flagged as high-risk.\n",
        "  - Improved Patient Outcomes : Reduces disease progression with early intervention.\n",
        "  - Cost Savings : Reduces unnecessary diagnostic procedures for low-risk patients.\n",
        "  - Data-Driven Decisions : Enables personalized treatment plans using insights from historical data.\n",
        "\n"
      ],
      "metadata": {
        "id": "uBOAXChsT4oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Dataset Info:\n",
        "â— Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "â— Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Question 6:   Write a Python program to:\n",
        "â— Load the Iris Dataset\n",
        "â— Train a Decision Tree Classifier using the Gini criterion\n",
        "â— Print the modelâ€™s accuracy and feature importances \"\"\"\n",
        "#Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data                   # Feature matrix\n",
        "y = iris.target                 # Target labels\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "#Split the data into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Create and train the Decision Tree Classifier using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "#Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjFYBEyvhLY5",
        "outputId": "d98ae3ad-2975-40c9-9e2b-984730c5d069"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 7:  Write a Python program to:\n",
        "â— Load the Iris Dataset\n",
        "â— Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree. \"\"\"\n",
        "#Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "#Split into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Train Decision Tree with max_depth = 3 (Pre-Pruned)\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "pruned_preds = pruned_tree.predict(X_test)\n",
        "pruned_accuracy = accuracy_score(y_test, pruned_preds)\n",
        "\n",
        "#Train Fully-grown Decision Tree (no depth limit)\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_preds = full_tree.predict(X_test)\n",
        "full_accuracy = accuracy_score(y_test, full_preds)\n",
        "\n",
        "#Print both accuracies\n",
        "print(f\"Accuracy of Pruned Tree (max_depth=3): {pruned_accuracy:.2f}\")\n",
        "print(f\"Accuracy of Fully-grown Tree        : {full_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GAfwX_dhkhq",
        "outputId": "4c095510-4106-4fd8-ba2e-4676e4d626ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Pruned Tree (max_depth=3): 1.00\n",
            "Accuracy of Fully-grown Tree        : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 8: Write a Python program to:\n",
        "â— Load the Boston Housing Dataset\n",
        "â— Train a Decision Tree Regressor\n",
        "â— Print the Mean Squared Error (MSE) and feature importances\"\"\"\n",
        "#Import necessary libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Load the Boston Housing dataset (via OpenML since load_boston is deprecated)\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "feature_names = X.columns\n",
        "\n",
        "#Split into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "#Predict and calculate Mean Squared Error (MSE)\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "#Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwroCOP3hzwJ",
        "outputId": "1bdd6c3e-10c3-4248-8830-05eba071b9d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.42\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0513\n",
            "ZN: 0.0034\n",
            "INDUS: 0.0058\n",
            "CHAS: 0.0000\n",
            "NOX: 0.0271\n",
            "RM: 0.6003\n",
            "AGE: 0.0136\n",
            "DIS: 0.0707\n",
            "RAD: 0.0019\n",
            "TAX: 0.0125\n",
            "PTRATIO: 0.0110\n",
            "B: 0.0090\n",
            "LSTAT: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 9: Write a Python program to:\n",
        "â— Load the Iris Dataset\n",
        "â— Tune the Decision Treeâ€™s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "â— Print the best parameters and the resulting model accuracy\"\"\"\n",
        "#Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "#Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up the Decision Tree model\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "#Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "#Use GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(estimator=dtree,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1)\n",
        "\n",
        "#Fit GridSearchCV on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "#Get the best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "#Evaluate the best model on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Test Set Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbR7_GVeiCbo",
        "outputId": "aa71d652-1030-4ae8-f25f-912c26955a3b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Test Set Accuracy: 1.00\n"
          ]
        }
      ]
    }
  ]
}